{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California is a state in the Western United States. California borders Oregon to the north, Nevada and Arizona to the east, the Mexican state of Baja California to the south; and has a coastline along the Pacific Ocean to the west.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=\"California is a state in the Western United States. California borders Oregon to the north, Nevada and Arizona to the east, the Mexican state of Baja California to the south; and has a coastline along the Pacific Ocean to the west.\"\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'california is a state in the western united states. california borders oregon to the north, nevada and arizona to the east, the mexican state of baja california to the south; and has a coastline along the pacific ocean to the west.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= data.lower()           # Converting the string to lower case to get uniformity\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 13,\n",
       " 2,\n",
       " 1,\n",
       " 14,\n",
       " 15,\n",
       " 6,\n",
       " 16,\n",
       " 2,\n",
       " 1,\n",
       " 17,\n",
       " 1,\n",
       " 18,\n",
       " 5,\n",
       " 19,\n",
       " 20,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 21,\n",
       " 6,\n",
       " 22,\n",
       " 4,\n",
       " 23,\n",
       " 24,\n",
       " 1,\n",
       " 25,\n",
       " 26,\n",
       " 2,\n",
       " 1,\n",
       " 27]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])  ## convert sentance to word  \n",
    "encoded_data = tokenizer.texts_to_sequences([data])[0]\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# Getting the total number of words of the data.\n",
    "word2idx = tokenizer.word_index  ####### index number to every token ro word \n",
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'to': 2, 'california': 3, 'a': 4, 'state': 5, 'and': 6, 'is': 7, 'in': 8, 'western': 9, 'united': 10, 'states': 11, 'borders': 12, 'oregon': 13, 'north': 14, 'nevada': 15, 'arizona': 16, 'east': 17, 'mexican': 18, 'of': 19, 'baja': 20, 'south': 21, 'has': 22, 'coastline': 23, 'along': 24, 'pacific': 25, 'ocean': 26, 'west': 27}\n"
     ]
    }
   ],
   "source": [
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1        # Adding 1 to the vocab_size because the index starts from 1 not 0. This will make it uniform when using it further\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(1,len(encoded_data)):\n",
    "    abc=encoded_data[i-1:i+1]\n",
    "    sequences.append(abc)\n",
    "    \n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 7],\n",
       " [7, 4],\n",
       " [4, 5],\n",
       " [5, 8],\n",
       " [8, 1],\n",
       " [1, 9],\n",
       " [9, 10],\n",
       " [10, 11],\n",
       " [11, 3],\n",
       " [3, 12],\n",
       " [12, 13],\n",
       " [13, 2],\n",
       " [2, 1],\n",
       " [1, 14],\n",
       " [14, 15],\n",
       " [15, 6],\n",
       " [6, 16],\n",
       " [16, 2],\n",
       " [2, 1],\n",
       " [1, 17],\n",
       " [17, 1],\n",
       " [1, 18],\n",
       " [18, 5],\n",
       " [5, 19],\n",
       " [19, 20],\n",
       " [20, 3],\n",
       " [3, 2],\n",
       " [2, 1],\n",
       " [1, 21],\n",
       " [21, 6],\n",
       " [6, 22],\n",
       " [22, 4],\n",
       " [4, 23],\n",
       " [23, 24],\n",
       " [24, 1],\n",
       " [1, 25],\n",
       " [25, 26],\n",
       " [26, 2],\n",
       " [2, 1],\n",
       " [1, 27]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=array(sequences)\n",
    "x,y=sequences[:,0],sequences[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 4, 5, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 5, 8, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=to_categorical(y,num_classes=vocab_size)\n",
    "x = np.reshape(x, (len(x), 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 10)          280       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                12200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                1428      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13908 (54.33 KB)\n",
      "Trainable params: 13908 (54.33 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 3s 9ms/step - loss: 3.3322 - accuracy: 0.0250\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.3303 - accuracy: 0.1750\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3288 - accuracy: 0.2000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3271 - accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3256 - accuracy: 0.1750\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3241 - accuracy: 0.1750\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3225 - accuracy: 0.1750\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3208 - accuracy: 0.1750\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3193 - accuracy: 0.1750\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3175 - accuracy: 0.1750\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.3159 - accuracy: 0.1750\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.3143 - accuracy: 0.1750\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3122 - accuracy: 0.1750\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 3.3107 - accuracy: 0.1750\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.3086 - accuracy: 0.1750\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.3067 - accuracy: 0.1750\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.3047 - accuracy: 0.1750\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.3027 - accuracy: 0.1750\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.3003 - accuracy: 0.1750\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2982 - accuracy: 0.1750\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2956 - accuracy: 0.1750\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.2931 - accuracy: 0.1750\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 3.2906 - accuracy: 0.1750\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2878 - accuracy: 0.1750\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.2846 - accuracy: 0.1750\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2818 - accuracy: 0.1750\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2783 - accuracy: 0.1750\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2752 - accuracy: 0.1750\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.2715 - accuracy: 0.1750\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.2681 - accuracy: 0.1750\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2640 - accuracy: 0.1750\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2605 - accuracy: 0.1750\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2562 - accuracy: 0.1750\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 3.2519 - accuracy: 0.1750\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2474 - accuracy: 0.1750\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.2431 - accuracy: 0.1750\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2377 - accuracy: 0.1750\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.2325 - accuracy: 0.1750\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2276 - accuracy: 0.1750\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.2212 - accuracy: 0.1750\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.2154 - accuracy: 0.1750\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.2094 - accuracy: 0.1750\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.2027 - accuracy: 0.1750\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1949 - accuracy: 0.1750\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.1880 - accuracy: 0.1750\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1805 - accuracy: 0.1750\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.1723 - accuracy: 0.1750\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1633 - accuracy: 0.1750\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1540 - accuracy: 0.1750\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 3.1446 - accuracy: 0.1750\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 3.1349 - accuracy: 0.1750\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 0s/step - loss: 3.1237 - accuracy: 0.1750\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 3.1123 - accuracy: 0.1750\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.1005 - accuracy: 0.1750\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 3.0886 - accuracy: 0.1750\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0756 - accuracy: 0.1750\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0622 - accuracy: 0.1750\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 3.0481 - accuracy: 0.1750\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0341 - accuracy: 0.1750\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 3.0188 - accuracy: 0.1750\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 3.0026 - accuracy: 0.1750\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9885 - accuracy: 0.1750\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9724 - accuracy: 0.1750\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9558 - accuracy: 0.1750\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9389 - accuracy: 0.1750\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.9240 - accuracy: 0.1750\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.9072 - accuracy: 0.1750\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8898 - accuracy: 0.1750\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8735 - accuracy: 0.1750\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.8550 - accuracy: 0.1750\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.8376 - accuracy: 0.1750\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.8187 - accuracy: 0.1750\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.7985 - accuracy: 0.1750\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7802 - accuracy: 0.2000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.7595 - accuracy: 0.2000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7406 - accuracy: 0.2000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7221 - accuracy: 0.2000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.7035 - accuracy: 0.2250\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6841 - accuracy: 0.2250\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6680 - accuracy: 0.2250\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6504 - accuracy: 0.2250\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 2.6342 - accuracy: 0.2250\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6197 - accuracy: 0.2250\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 2.6031 - accuracy: 0.2500\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.5896 - accuracy: 0.2500\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5753 - accuracy: 0.2500\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5613 - accuracy: 0.2750\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.5478 - accuracy: 0.2750\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 2.5345 - accuracy: 0.2750\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5219 - accuracy: 0.2750\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.5093 - accuracy: 0.2750\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.4967 - accuracy: 0.2750\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.4839 - accuracy: 0.2750\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4721 - accuracy: 0.2750\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.4597 - accuracy: 0.2750\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4475 - accuracy: 0.2750\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.4355 - accuracy: 0.2750\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.4237 - accuracy: 0.2750\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.4117 - accuracy: 0.2750\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.3994 - accuracy: 0.2750\n"
     ]
    }
   ],
   "source": [
    "# model.summary()                                       # We can know about the shape of the model\n",
    "r = model.fit(x,y,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(model,tokenizer,enter_text,n_pred):\n",
    "    in_text,result=enter_text,enter_text\n",
    "    for _ in range(n_pred):\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]  # convert to squences \n",
    "        encoded = np.reshape(encoded, (1,1,1))\n",
    "        predicted = np.argmax(model.predict(encoded), axis=-1)\n",
    "        out_word=''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if  index==predicted:    # Predicted sequence = Index Data\n",
    "                out_word = word\n",
    "                break\n",
    "        in_text,result=out_word,result + ' ' + out_word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "the mexican the mexican the mexican the mexican\n"
     ]
    }
   ],
   "source": [
    "print(predict_words(model,tokenizer,'the',7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
